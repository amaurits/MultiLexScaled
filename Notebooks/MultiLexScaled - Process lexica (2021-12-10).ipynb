{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLexScaled: Process lexica (2021-12-10)\n",
    "\n",
    "_by A. Maurits van der Veen_  \n",
    "\n",
    "_Modification history:_  \n",
    "_2021-11-14 - initial extraction from longer, older notebook_   \n",
    "_2021-12-05 - clean-up and update for Github_   \n",
    "_2021-12-10 - double-check availability of all lexica at URLs indicated_  \n",
    "\n",
    "The sentiment analysis method MultiLexScaled uses 8 widely used, publicly available, lexica. Not all of these are in the same format to start with. This notebook takes the format in which these are available online, and converts each to a csv file.\n",
    "\n",
    "The lexica are processed in the following (alphabetical) order:\n",
    "- HuLiu  \n",
    "- labMT \n",
    "- LexicoderSD  \n",
    "- MPQA  \n",
    "- NRC  \n",
    "- SentiWordNet (filtered0.1)\n",
    "- SO-CAL (including the intensifier dictionary used in valence calculation)\n",
    "- WordStat\n",
    "\n",
    "Most of the lexica receive modifications, which are marked by modifying the filename (either adding `_filtered` or simply a capital `X`:\n",
    "- HuLiu_lexiconX\n",
    "- labMT_lexicon_filtered\n",
    "- LSD_lexiconX\n",
    "- SWN_lexicon_filtered0.1\n",
    "- SO-CAL_lexiconX & SO-CAL_modifiersX\n",
    "- WordStat_lexicon2X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set-up\n",
    "\n",
    "Import code modules and specify project folder path. Also define some useful lexicon-editing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAIRfolder = '/Users/noname/STAIR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code files to import\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Print summary version info (for fuller info, simply print sys.version)\n",
    "print('You are using python version {}.'.format(sys.version.split()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathname to contain all the individual lexicon files\n",
    "SAfolder = STAIRfolder + 'Corpora/Lexica/English/MultiLexScaled/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1. Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_lex(lex, fixdict):\n",
    "    \"\"\"Fix a lexicon, by replacing each key in fixdict with the corrected key.\n",
    "    \n",
    "    Used to fix apparent unintentional spelling errors in some of the sentiment lexicon keys.\n",
    "    \"\"\"\n",
    "    for oldlexkey, newlexkey in fixdict.items():\n",
    "        if oldlexkey in lex:\n",
    "            lexval = lex[oldlexkey]\n",
    "            del lex[oldlexkey]\n",
    "            if newlexkey != '':\n",
    "                lex[newlexkey] = lexval\n",
    "                print('Replaced {} by {}'.format(oldlexkey, newlexkey))\n",
    "            else:\n",
    "                print('Deleted {}'.format(oldlexkey))\n",
    "           \n",
    "    return lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsumed(origx, words, wild='*', report=True):\n",
    "    \"\"\"See whether origx is subsumed by a wildcarded entry in words.\"\"\"\n",
    "    if origx[-1] == wild:\n",
    "        x = origx[:-2] + wild\n",
    "    else:\n",
    "        x = origx + wild\n",
    "    while len(x) > 1:\n",
    "        if x in words:\n",
    "            if report:\n",
    "                print(x, 'subsumes', origx)\n",
    "            return x\n",
    "        else:\n",
    "            x = x[:-2] + wild\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_removesubsumed(lex):\n",
    "    \"\"\"Remove all entries in a lexicon that are subsumed by a wildcard entry with the same valence.\n",
    "    \n",
    "    Note that sometimes these are unintended wildcard matches. \n",
    "    For example: 'terrifi*' (WordStat) is negative because intended for\n",
    "    'terrified', 'terrifies', etc. However, it also subsumes 'terrific*',\n",
    "    which is positive. \n",
    "    \n",
    "    In such cases, we want to look for the longest match first, which is\n",
    "    indeed how our wildcard matching function operates. Therefore, we do not\n",
    "    delete such subsumptions.\n",
    "    \"\"\"\n",
    "    entries2delete = []\n",
    "    for key, val in lex.items():\n",
    "        subsumption = subsumed(key, lex, wild='*', report=True)  # set report to False for quiet operation\n",
    "        if subsumption:\n",
    "            if lex[subsumption] == val:\n",
    "                entries2delete.append(key)\n",
    "            else:\n",
    "                print('{} and {} have different valences => keeping both'.format(subsumption, key))\n",
    "            \n",
    "    if len(entries2delete) > 1:\n",
    "        print('Found {} subsumed entries; deleting now'.format(len(entries2delete)))\n",
    "        for entry in entries2delete:\n",
    "            del lex[entry]\n",
    "        \n",
    "    return lex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hu & Liu\n",
    "\n",
    "Sentiment lexicon is available here: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html (look for \"opinion lexicon\"). Downloading the lexicon should produce a folder `opinion-lexicon-english` which contains be 2 files: `positive-words.txt` and `negative-words.txt`. Please cite the associated paper:\n",
    "\n",
    "- Minqing Hu and Bing Liu. \"Mining and summarizing customer reviews.\" Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD-2004, full paper), Seattle, Washington, USA, Aug 22-25, 2004. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importHuLiu(lexiconfolder, posfile, negfile, savepickle=False, savetext=True):\n",
    "    \"\"\"Import and merge sentiment lexica from Bing Liu\n",
    "\n",
    "    The opinion lexicon rar archive unpacks as a folder opinion-lexicon-English\n",
    "    with the files positive-words.txt and negative-words.txt inside.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    import pickle\n",
    "\n",
    "    words = {}\n",
    "\n",
    "    # Read in the positive words\n",
    "    with open(lexiconfolder + posfile,'r', errors='ignore') as infile:\n",
    "        for line in infile.readlines():\n",
    "            if line[0] != ';' and line.strip() != '':\n",
    "                words[line.strip()] = 1\n",
    "    poscount = len(words)\n",
    "                \n",
    "    # Read in the negative words\n",
    "    with open(lexiconfolder + negfile,'r', errors='ignore') as infile:\n",
    "        for line in infile.readlines():\n",
    "            if line[0] != ';' and line.strip() != '':\n",
    "                words[line.strip()] = -1\n",
    "\n",
    "    # print(list(words.items())[:20])\n",
    "    print('Loaded {} positive and {} negative words, for a total of {} words.'.format(\n",
    "            poscount, len(words)-poscount, len(words)))\n",
    "          \n",
    "    if savepickle:\n",
    "        with open(lexiconfolder + 'HuLiu_lexicon.pkl', 'wb') as outfile:\n",
    "            pickle.dump(words, outfile)\n",
    "    if savetext:\n",
    "        with open(lexiconfolder + 'HuLiu_lexicon.csv', 'w') as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexiconfolder = SAfolder + 'HuLiu/opinion-lexicon-English/'\n",
    "posfile = 'positive-words.txt'\n",
    "negfile = 'negative-words.txt'\n",
    "\n",
    "huliulex = importHuLiu(lexiconfolder, posfile, negfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lexicon has 'bull****' as a euphemism for 'bullshit'. However, our system\n",
    "# would recognize the asterisks as wildcards, so delete this entry.\n",
    "\n",
    "del huliulex['bull****']\n",
    "\n",
    "# In addition, it has naïve, but the ï does not come through correctly in the loading, so fix that\n",
    "\n",
    "huliulex['naïve'] = huliulex['nave']\n",
    "del huliulex['nave']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the updated file\n",
    "\n",
    "with open(lexiconfolder + 'HuLiu_lexiconX.csv', 'w') as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(huliulex.items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(huliulex), \n",
    "        sum([1 for x in huliulex if huliulex[x] > 0]),\n",
    "        sum([1 for x in huliulex if huliulex[x] < 0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. labMT (filtered)\n",
    "\n",
    "labMT stands for \"language analysis by Mechanical Turk\". The lexicon is available here: https://github.com/ryanjgallagher/shifterator/tree/master/shifterator/lexicons/labMT (use the file `labMT_English.tsv`). Please cite the associated paper:\n",
    "\n",
    "- Dodds, Peter Sheridan, Kameron Decker Harris, Isabel M. Kloumann, Catherine A. Bliss, and Christopher M. Danforth. \"Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter.\" PLoS ONE 6, no. 12 (2011).\n",
    "\n",
    "labMT is centered around 5, rather than 0. Subtract 5 to get a 0-centered lexicon.\n",
    "\n",
    "Words right around 0 are effectively neutral in valence, so filter out all entries with an absolute value for valence less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT = {}\n",
    "\n",
    "labMTfolder = SAfolder + 'labMT/'\n",
    "with open(labMTfolder + 'labMT_English.tsv', 'r') as labMTfile:\n",
    "    labreader = csv.reader(labMTfile, delimiter='\\t')\n",
    "    for row in labreader:\n",
    "        labMT[row[0]] = float(row[1]) - 5\n",
    "\n",
    "len(labMT)  # Should be 10,222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMTfiltered = {key: val for key, val in labMT.items() if abs(val) >= 1}\n",
    "len(labMTfiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(labMTfolder + 'labMT_lexicon_filtered.csv', 'w') as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(labMTfiltered.items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(labMTfiltered), \n",
    "        sum([1 for x in labMTfiltered if labMTfiltered[x] > 0]),\n",
    "        sum([1 for x in labMTfiltered if labMTfiltered[x] < 0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lexicoder Sentiment Dictionary\n",
    "\n",
    "The Lexicoder Sentiment Dictionary (LSD) was developed by Lori Young and Stuart Soroka. It is available at http://www.snsoroka.com/data-lexicoder/ and is part of the larger Lexicoder system, which also involves text preprocessing as well as some language substitution. Please cite the accompanying paper: \n",
    "\n",
    "- Young, Lori, and Stuart Soroka. \"Affective news: The automated coding of sentiment in political texts.\" Political Communication 29.2 (2012): 205-231.\n",
    "\n",
    "We use the August 2015 version. There are a few multi-word entries in the list; we ignore those. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importLexicoder(lexfolder, lexfile, lexfile_negated, savepickle=False, savetext=True, oldformat=False):\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    words, negwords = {}, {}\n",
    "    \n",
    "    # Read in the non-negated values\n",
    "    curcat = ''\n",
    "    curval = 0\n",
    "    with open(lexfolder + lexfile, 'r') as infile:\n",
    "        for line in infile.readlines():\n",
    "            if oldformat: \n",
    "                line = line.lower()\n",
    "            if line[0] == '+' or (oldformat and line[0] != '\\t'):\n",
    "                curcat = 'positive' if 'positive' in line else 'negative'\n",
    "                curval = 1 if curcat == 'positive' else -1\n",
    "            else:\n",
    "                aWord = line.strip().lower()\n",
    "                if oldformat:\n",
    "                    aWord = aWord[:-4]  # remove ' (1)' at end of each line\n",
    "                if ' ' in aWord:\n",
    "                    print('Phrase: {} -> skipping'.format(aWord))\n",
    "                    continue\n",
    "                words[aWord] = curval\n",
    "\n",
    "    # Read in the negated values; see which, if any, are new\n",
    "    curcat = ''\n",
    "    with open(lexfolder + lexfile_negated, 'r') as infile:\n",
    "        for line in infile.readlines():\n",
    "            if oldformat:\n",
    "                line = line.lower()\n",
    "            if line[0] == '+' or (oldformat and line[0] != '\\t'):\n",
    "                curcat = 'positive' if 'positive' in line else 'negative'\n",
    "                curval = 1 if curcat == 'positive' else -1\n",
    "                continue\n",
    "            linesplit = line.split()\n",
    "            if oldformat:\n",
    "                linesplit = linesplit[:-1]\n",
    "            if linesplit[0] == 'not' and len(linesplit) == 2:  # skip phrases\n",
    "                aWord = linesplit[1].lower()\n",
    "                if aWord not in words:\n",
    "                    negwords[aWord] = curval\n",
    "                    # print(\"New one from negatives: {} ({})\".format(aWord, curval))\n",
    "                else:\n",
    "                    if words[aWord] != curval:  # should not happen!\n",
    "                        print('Warning: {} has valence {} in original, but {} in negated file'.format(\n",
    "                                aWord, words[aWord], curval))\n",
    "            else: # phrase not beginning 'not' -> skip\n",
    "                # print('{} = negative file phrase or entry not beginning \"not\": -> skipping'.format(line))\n",
    "                continue\n",
    "                \n",
    "    # Identify the ones that are in the basic file but not in the negated one\n",
    "    # print(\"Words in the main file but not the negated file:\")\n",
    "    # print([x for x in words.keys() if x not in negwords])\n",
    "    \n",
    "    # Identify the ones that are in the negated list but not the basic one\n",
    "    negwords_pos = [word for word, val in negwords.items() if val == 1]   \n",
    "    negwords_neg = [word for word, val in negwords.items() if val == -1]\n",
    "    if len(negwords_pos) > 0:\n",
    "        print('\\nPositive-valence words in the negated file only: {}'.format(', '.join(negwords_pos)))\n",
    "    if len(negwords_pos) > 0:\n",
    "        print('\\nNegative-valence words in the negated file only: {}'.format(', '.join(negwords_neg)))\n",
    "        \n",
    "    # Add them to the full list\n",
    "    for word, val in negwords.items():\n",
    "        if word not in words:\n",
    "            words[word] = val\n",
    "    \n",
    "    print(\"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "            len(words), \n",
    "            sum([words[x] for x in words if words[x] == 1]),\n",
    "            -sum([words[x] for x in words if words[x] == -1])))\n",
    "\n",
    "    # Save and return the lexicon\n",
    "    if savepickle:\n",
    "        with open(lexfolder + 'LexicoderDictionary.pkl', 'wb') as LSDout:\n",
    "            pickle.dump(words, LSDout)\n",
    "\n",
    "    if savetext:\n",
    "        with open(lexfolder + 'LSD_lexicon.csv', 'w') as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdfolder = SAfolder + 'Lexicoder/LSDaug2015/'\n",
    "lsdfile = 'LSD2015.lc3'\n",
    "lsdfile_negated = 'LSD2015_NEG.lc3'\n",
    "\n",
    "lsd_lex = importLexicoder(lsdfolder, lsdfile, lsdfile_negated, savepickle=False, savetext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter out subsumptions (only if they have the same valence)\n",
    "\n",
    "lsd_lex = lex_removesubsumed(lsd_lex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the changed lexicon\n",
    "\n",
    "with open(lsdfolder + 'LSD_lexiconX.csv', 'w') as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(lsd_lex.items())))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(lsd_lex), \n",
    "        sum([lsd_lex[x] for x in lsd_lex if lsd_lex[x] == 1]),\n",
    "        -sum([lsd_lex[x] for x in lsd_lex if lsd_lex[x] == -1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MPQA (Multi-Perspective Question Answering)\n",
    "\n",
    "We use the lexicon associated with OpinionFinder 2.0, available at http://mpqa.cs.pitt.edu/opinionfinder/opinionfinder_2/. (Note that this is slightly different from the subjectivity lexicon available on the same website at: http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/.) We use the field `mpqapolarity`, which has values strongneg, weakneg, weakpos, and strongpos; we translate those values to -1.0, -0.5, 0.5, and 1.0. Two additional values -- neutral and both -- are ignored. If words occur in multiple parts of speech, their valence will be averaged.\n",
    "\n",
    "Please cite the associated paper:\n",
    "\n",
    "- Theresa Wilson, Janyce Wiebe, and Paul Hoffmann (2005). Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis. Proc. of HLT-EMNLP-2005. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importMPQA(lexiconfolder, lexiconfile, savepickle=False, savetext=True):\n",
    "    \"\"\"Import the length-1 subjectivity clues files from MPQA; convert to dictionary.\n",
    "\n",
    "    Original sentiment assessments are strongpos, weakpos, weakneg, strongneg\n",
    "    Assign values 1, 0.5, -0.5, -1.\n",
    "    Average across different word usages (parts-of-speech)\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    opinionvals = {'strongpos': 1, 'weakpos': 0.5, 'weakneg': -0.5, 'strongneg': -1,\n",
    "                   'neutral': 0, 'both': 0}\n",
    "\n",
    "    with open(lexiconfolder + lexiconfile, 'r') as in1:\n",
    "        cluesdata = in1.readlines()\n",
    "\n",
    "    lexicon = {}\n",
    "    wordcount = 0\n",
    "    for counter, wordinfo in enumerate(cluesdata):\n",
    "        if wordinfo[0] != '#':  # skip comment lines (should not be present)\n",
    "            wordsplit = wordinfo.split()\n",
    "            \n",
    "            # Skip any terms not of length 1\n",
    "            termlength = [x[-1] for x in wordsplit if x[:3] == 'len']\n",
    "            if len(termlength) > 0 and termlength[0] == '1':\n",
    "                \n",
    "                # Extract word and polarity; these are not always the same location, so search\n",
    "                theword = [x[6:] for x in wordsplit if x[:5] == 'word1']\n",
    "                thesent = [x[13:] for x in wordsplit if x[:12] == 'mpqapolarity']\n",
    "                if len(theword) > 0 and len(thesent) > 0:\n",
    "                    \n",
    "                    # Store multiple uses of the same word (varies by POS)\n",
    "                    if theword[0] in lexicon:\n",
    "                        lexicon[theword[0]].append(opinionvals[thesent[0]])\n",
    "                    else:\n",
    "                        lexicon[theword[0]] = [opinionvals[thesent[0]],]\n",
    "                        wordcount += 1\n",
    "\n",
    "    # Assign valence by averaging across multiple valences for the same word\n",
    "    # At the same time, filter out words whose average valence is 0\n",
    "    newlexicon = {key: sum(val)/float(len(val)) for key, val in lexicon.items() if abs(sum(val)) > 0}\n",
    "\n",
    "    print(\"Total lines: %d; unique words: %d, in lexicon: %d\" % \\\n",
    "          (counter, wordcount, len(newlexicon)))\n",
    "\n",
    "    if savetext:\n",
    "        with open(lexiconfolder + 'MPQA_lexicon.csv', 'w') as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(newlexicon.items())))\n",
    "\n",
    "    if savepickle:\n",
    "        with open(lexiconfolder + 'MPQA_lexicon.pkl', 'wb') as outfile:\n",
    "            pickle.dump(newlexicon, outfile)\n",
    "\n",
    "    return newlexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpqafolder = SAfolder + 'MPQA 2.0/opinionfinderv2.0/lexicons/'\n",
    "mpqafile = 'subjclueslen1polar.tff'\n",
    "\n",
    "mpqa_lex = importMPQA(mpqafolder, mpqafile, savepickle=False, savetext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(mpqa_lex), \n",
    "        sum([1 for x in mpqa_lex if mpqa_lex[x] > 0]),\n",
    "        sum([1 for x in mpqa_lex if mpqa_lex[x] < 0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. NRC (Canadian National Research Council)\n",
    "\n",
    "The NRC lexicon is a LIWC-style lexicon, with values for multiple categorie: each line contains a word, a category, and the value for that category. We use version 0.92 of the lexicon, which is available here: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm (the filename is `NRC-Emotion-Lexicon-Wordlevel-v0.92.txt`). Please cite the associated paper:\n",
    "\n",
    "- Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\n",
    "\n",
    "There are 81 words in the lexicon that are listed as both positive and negative. Through 2021 we included these as positive. In the current version we delete them, in parallel with the way we treat the 'both' options in the MPQA lexicon.\n",
    "\n",
    "There are also 12 words (4 positive, 8 negative) that appear to have been removed from the lexicon since 2015. They are listed at the end of this section for reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importNRC(nrcfolder, nrcfile, savepickle=False, savetext=True):\n",
    "    \"\"\"Import NRC sentiment dictionary.\"\"\"\n",
    "    import csv, pickle\n",
    "\n",
    "    words = {}\n",
    "    doublewords = []\n",
    "    \n",
    "    with open(nrcfolder + nrcfile,'r') as infile:\n",
    "        for row in csv.reader(infile, delimiter='\\t'):\n",
    "            if len(row) > 0 and row[0] != '':\n",
    "                \n",
    "                word = row[0].strip()\n",
    "                cat = row[1].strip()\n",
    "                val = int(row[2])\n",
    "                \n",
    "                if cat == 'negative' and val == 1:  # should never happen\n",
    "                    if word in words:\n",
    "                        print('{} was in there already: was {}, now {}.'.format(word, words[word], val))\n",
    "                    words[word] = -1\n",
    "                        \n",
    "                elif cat == 'positive' and val == 1:  # happens when word is in list as both negative and positive\n",
    "                    if word in words:\n",
    "                        doublewords.append(word)\n",
    "                        del words[word]\n",
    "                    else:\n",
    "                        words[word] = 1\n",
    "                        \n",
    "                else:  # other categories: emotions\n",
    "                    pass\n",
    "\n",
    "    print(\"Words: {}, positive: {}, negative: {}, both (deleted): {}\".format(\n",
    "          len(words), len([1 for w in words if words[w] == 1]),\n",
    "          len([1 for w in words if words[w] == -1]),\n",
    "          len(doublewords)))\n",
    "\n",
    "    if savepickle:\n",
    "        with open(nrcfolder + 'NRC_lexicon.pkl', 'wb') as outfile:\n",
    "            pickle.dump(words, outfile)\n",
    "\n",
    "    if savetext:\n",
    "        with open(nrcfolder + 'NRC_lexicon.csv', 'w') as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "\n",
    "    return words, doublewords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRCfolder = SAfolder + 'NRC/NRC-Emotion-Lexicon-v0.92/'\n",
    "NRCfile = 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "\n",
    "nrclex, posnegwords = importNRC(NRCfolder, NRCfile, savepickle=False, savetext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(nrclex), \n",
    "        sum([1 for x in nrclex if nrclex[x] > 0]),\n",
    "        sum([1 for x in nrclex if nrclex[x] < 0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SentiWordNet\n",
    "\n",
    "SentiWordNet is based on the WordNet semantic network. It is available at https://github.com/aesuli/SentiWordNet. Please cite the associated paper:\n",
    "\n",
    "- Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. \"Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining.\" Lrec. Vol. 10. No. 2010. 2010.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importSWN(swnfolder, swnfile, savepickle=False, savetext=True):\n",
    "    \"\"\"Import SentiWordNet as sentiment analysis lexicon.\n",
    "\n",
    "    Produce a plain sentiment dictionary by separating out members of each synset.\n",
    "    For polysemous words, simply average valences.\n",
    "\n",
    "    File format: tab-separated\n",
    "    - POS (a, n, ...)\n",
    "    - WordNet ID\n",
    "    - positive score (0-1)\n",
    "    - negative score (0-1)\n",
    "    - synset (list of members)\n",
    "    - gloss (verbalization of meaning)\n",
    "\n",
    "    Synset members: space-separated; each word with the suffix '#n' where n is the sequence\n",
    "    number for the synset memberships of the same word.\n",
    "\n",
    "    Ignore multi-word phrases. The dataset contains 83499 words, of which 20,099 positive,\n",
    "    20,698 negative (but 9,783 of the positive/negative words are both). If we sum the positive\n",
    "    and negative, there are 29,502 words with a non-zero sum. \n",
    "    \n",
    "    There is commented-out code in here for an alternative way of generating a lexicon.\n",
    "    Un-comment if needed. The alterantive way is to ignore all the zero-values assigned \n",
    "    to a word's positive or negative scores and include the non-zero values only. These\n",
    "    data are tracked in the variable nonzerovals and compiled into the lexicon polarity2.    \n",
    "    \"\"\"\n",
    "    import csv, pickle\n",
    "    from operator import itemgetter\n",
    "\n",
    "    # Initialize dictionaries\n",
    "    synsets, wordpos, posvals, negvals, nonzerovals, polarity, polarity2 = {}, {}, {}, {}, {}, {}, {}\n",
    "\n",
    "    with open(swnfolder + swnfile,'r') as infile:\n",
    "        \n",
    "        # Run down synsets\n",
    "        for row in csv.reader(infile, delimiter='\\t'):\n",
    "            if len(row) > 0 and row[0] != '' and row[0][0] != '#':  # skip comments &c.\n",
    "\n",
    "                pos = row[0].strip()\n",
    "                id = row[1].strip()\n",
    "                posval = float(row[2])\n",
    "                negval = float(row[3])\n",
    "                synsetraw = row[4].split()\n",
    "                \n",
    "                # Run down members of each synset\n",
    "                for term in synsetraw:\n",
    "                    word = term[:-2]  # skip the synset number (#1, #2, etc.)\n",
    "                    if '_' not in word:  # ignore multi-word phrases\n",
    "                        \n",
    "                        # Track non-zero values\n",
    "                        # if posval != 0:\n",
    "                        #     if word in nonzerovals:\n",
    "                        #         nonzerovals[word].append(posval)\n",
    "                        #     else:\n",
    "                        #         nonzerovals[word] = [posval,]\n",
    "                        # if negval != 0:\n",
    "                        #     if word in nonzerovals:\n",
    "                        #         nonzerovals[word].append(-negval)\n",
    "                        #     else:\n",
    "                        #         nonzerovals[word] = [-negval,]\n",
    "\n",
    "                        # Weight zero-values as equal to non-zero values\n",
    "                        if word not in synsets:  # first or only meaning of word\n",
    "                            synsets[word] = [id,]\n",
    "                            wordpos[word] = [pos,]\n",
    "                            posvals[word] = posval\n",
    "                            negvals[word] = negval\n",
    "                                    \n",
    "                        else:  # polysemous word\n",
    "                            nrmeanings = len(synsets[word])\n",
    "                            oldposvals = posvals[word] * nrmeanings\n",
    "                            oldnegvals = negvals[word] * nrmeanings\n",
    "                            synsets[word].append(id)\n",
    "                            if pos not in wordpos[word]:\n",
    "                                wordpos[word].append(pos)\n",
    "                            posvals[word] = (oldposvals + posval)/  \\\n",
    "                                            float(nrmeanings + 1)\n",
    "                            negvals[word] = (oldnegvals + negval)/  \\\n",
    "                                            float(nrmeanings + 1)\n",
    "                            \n",
    "    # Combine positive and negative\n",
    "    # Note: this treats 0 values as equal in weight to non-zero values\n",
    "    for word, val in posvals.items():\n",
    "        sumvals = val - negvals[word]\n",
    "        if sumvals != 0:\n",
    "            polarity[word] = sumvals\n",
    "    \n",
    "    # We could also focus on non-zero values only:\n",
    "    # for word, vals in nonzerovals.items():\n",
    "    #     avgval = sum(vals)/len(vals)\n",
    "    #     if avgval != 0:\n",
    "    #         polarity2[word] = avgval\n",
    "    \n",
    "    # Report basic data\n",
    "    print(\"Words: {}, positive: {}, negative: {}, both: {}, sum non-0: {}\".format(\n",
    "          len(synsets),\n",
    "          len([1 for w in posvals if posvals[w] > 0]),\n",
    "          len([1 for w in negvals if negvals[w] > 0]),\n",
    "          len([1 for w in posvals if posvals[w] > 0 and negvals[w] > 0]),\n",
    "          len(polarity),\n",
    "          # len(polarity2),        \n",
    "          ))\n",
    "    \n",
    "    if savepickle:\n",
    "        with open(swnfolder + 'SWNlex.pkl', 'wb') as outfile:\n",
    "            pickle.dump((polarity, posvals, negvals, synsets, wordpos), outFile)\n",
    "            \n",
    "    if savetext:\n",
    "        with open(swnfolder + 'SWN_lexicon.csv', 'w') as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(polarity.items())))\n",
    "        # with open(swnfolder + 'SWN_lexicon2.csv', 'w') as outfile:\n",
    "        #     outwriter = csv.writer(outfile)\n",
    "        #     outwriter.writerows(sorted(list(polarity2.items())))\n",
    "            \n",
    "    return polarity # , polarity2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWNfolder = SAfolder + 'SWN/'\n",
    "SWNfile = 'SentiWordNet_3.0.0.txt'\n",
    "\n",
    "# swnlex, swnlex2 = importSWN(SWNfolder, SWNfile, savepickle=False, savetext=True)\n",
    "swnlex = importSWN(SWNfolder, SWNfile, savepickle=False, savetext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all the values that are very close to 0 (use 0.1 as the cut-off)\n",
    "\n",
    "cutoff = 0.1\n",
    "swn_filtered = {key: val for key, val in swnlex.items() if abs(val) >= cutoff}\n",
    "len(swn_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated version\n",
    "with open(SWNfolder + 'SWN_lexicon_filtered0.1.csv', 'w') as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(swn_filtered.items())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLexicon length: {} ({} positive & {} negative)\".format(\n",
    "        len(swn_filtered), \n",
    "        sum([1 for x in swn_filtered if swn_filtered[x] > 0]),\n",
    "        sum([1 for x in swn_filtered if swn_filtered[x] < 0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. SO-CAL\n",
    "\n",
    "The SO-CAL dictionaries are available at https://github.com/sfu-discourse-lab/SO-CAL/tree/master/Resources/dictionaries/English. Please cite the associated paper:\n",
    "\n",
    "- Taboada, Maite, Julian Brooke, Milan Tofiloski, Kimberly Voll and Manfred Stede (2011) Lexicon-Based Methods for Sentiment Analysis. Computational Linguistics 37 (2): 267-307.\n",
    "\n",
    "Note: SO-CAL's dictionaries provide only the singular for nouns and the infinitive for verbs. We expand those lists by adding plurals as well as basic verb conjugations. For this, we use the python module `pattern` ( https://github.com/clips/pattern).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importSOCAL(socalfolder, socalnames, savepickle=False, savetext=True):\n",
    "    \"\"\"Import and pre-process SO-CAL sentiment lexicon\n",
    "\n",
    "    Expects 5 separate files in the socalfolder, with names specified in the\n",
    "    dictionary socalnames, which should have the keys 'adjective', 'adverb',\n",
    "    'noun', and 'verb', plus 'intensifier' for the separate intensifiers list.\n",
    "\n",
    "    We ignore part-of-speech, and we expand nouns (pluralize) and verbs\n",
    "    (singular, plural, prsent, past, etc.) to combine into a single dictionary.\n",
    "    We use the pattern module for these expansions.\n",
    "    When words appear in multiple sub-lists, simply average their valence.\n",
    "\n",
    "    Each file contains 1 word per line, tab-separated from its valence.\n",
    "    For the 4 POS categories, these are words to be added to the lexicon; for the last,\n",
    "    the 'valence' is the multiplier to use when the word precedes a valence\n",
    "    word.\n",
    "\n",
    "    Note: A number of these 'words' are multi-word phrases, some with\n",
    "    generic wildcards and POS tags (e.g. \"(bowl)_#PER?#_over\" ). These are all separated\n",
    "    by hyphens or underscores; skip them here, to keep single words only in the lexicon.\n",
    "\n",
    "    Note 2: some entries are in the verb dictionary twice, and their valence is automatically averaged\n",
    "    - ameliorate, at 1 and 2\n",
    "    - appall, at -3 and -5\n",
    "    - befriend (both at 1)\n",
    "    - belie, at -2 and -3\n",
    "    - bug, at -1 and -2\n",
    "    - enthrall (both at 3)\n",
    "    - extol, at 2 and 3\n",
    "    - gladden, at 2 and 3\n",
    "    - loathe at -4 and loath at -5\n",
    "    - misunderstand (both at -1), plus misunderstood, also at -1\n",
    "    - quibble (both at -1)\n",
    "    - uplift, at 2 and 3\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    from operator import itemgetter\n",
    "    from pattern.en import pluralize\n",
    "    import pickle\n",
    "\n",
    "    words, counts = {}, {}\n",
    "\n",
    "    # Read in the nouns; handle plurals\n",
    "    counter = 0\n",
    "    with open(socalfolder + socalnames['noun'], 'r', errors='ignore') as infile:\n",
    "        inreader = csv.reader(infile, delimiter='\\t')\n",
    "        for row in inreader:\n",
    "            if len(row) == 2:  # only take lines with 2 entries\n",
    "                counter += 1\n",
    "                theword = row[0]\n",
    "                theval = int(row[1])\n",
    "                words, counts = \\\n",
    "                    updatewordscounts(theword, theval, words, counts)\n",
    "                words, counts = \\\n",
    "                    updatewordscounts(pluralize(theword), theval, words, counts)\n",
    "    lensofar = len(words)\n",
    "    print(\"Processed {} nouns; total count (singular + plural): {}\".format(counter, lensofar))\n",
    "\n",
    "    # Read in the verbs; handle conjugation\n",
    "    with open(socalfolder + socalnames['verb'], 'r') as infile:\n",
    "        inreader = csv.reader(infile, delimiter='\\t')\n",
    "        for row in inreader:\n",
    "            if len(row) == 2:\n",
    "                theval = int(row[1])\n",
    "                for verbtense in alltenses(row[0]):\n",
    "                    words, counts = \\\n",
    "                        updatewordscounts(verbtense, theval, words, counts)\n",
    "    print(\"Verbs (incl. conjugations):\", len(words) - lensofar)\n",
    "    lensofar = len(words)\n",
    "\n",
    "    # Read in adjectives & adverbs: no special treatment\n",
    "    for pos in ('adjective', 'adverb'):\n",
    "        with open(socalfolder + socalnames[pos], 'r', errors='ignore') as infile:\n",
    "            inreader = csv.reader(infile, delimiter='\\t')\n",
    "            for row in inreader:\n",
    "                if len(row) == 2:\n",
    "                    words, counts = \\\n",
    "                        updatewordscounts(row[0], int(row[1]), words, counts)\n",
    "    print(\"Adjectives & adverbs:\", len(words) - lensofar)\n",
    "\n",
    "    # Remove phrases, to keep single words only\n",
    "    terms2delete = []\n",
    "    for term, val in words.items():\n",
    "        if '_' in term:  # redundant: or '(' in term or '[' in term\n",
    "            terms2delete.append(term)\n",
    "            # print(term)\n",
    "    print('Deleting {} phrases (identified by underscores)'.format(len(terms2delete)))\n",
    "    for term in terms2delete:\n",
    "        del words[term]\n",
    "\n",
    "    print('Total lexicon length:', len(words))\n",
    "\n",
    "    # Now read in the modifiers\n",
    "    mods = {}\n",
    "    with open(socalfolder + socalnames['intensifier'], 'r') as infile:\n",
    "        inreader = csv.reader(infile, delimiter='\\t')\n",
    "        for row in inreader:\n",
    "            if len(row) == 2:\n",
    "                mods[row[0]] = float(row[1])\n",
    "    print(\"Modifiers:\", len(mods))\n",
    "\n",
    "    # Print some output to double-check everything looks right\n",
    "    # print(list(words.items())[:20])\n",
    "    # print(\"Total nr. of words\", len(words))\n",
    "    # print(list(mods.items())[:20])\n",
    "    # print(\"Total nr. of modifiers\", len(mods))\n",
    "    # dupes = [(word, val) for word, val in counts.items() if val > 1]\n",
    "    # print(sorted(dupes, key=itemgetter(1), reverse=True))\n",
    "    # print(\"Total nr. of words encountered more than once\", len(dupes))\n",
    "\n",
    "    # Save SO-CAL dictionary in a text or pickle file\n",
    "    if savetext:\n",
    "        with open(socalfolder + 'SO-CAL_lexicon.csv', 'w') as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "        with open(socalfolder + 'SO-CAL_modifiers.csv', 'w') as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(mods.items())))\n",
    "    if savepickle:\n",
    "        with open(socalfolder + 'SO-CAL.pkl', 'wb') as outfile:\n",
    "            pickle.dump((words, mods, counts), outfile)\n",
    "\n",
    "    return words, mods\n",
    "\n",
    "\n",
    "def updatewordscounts(word, val, words, counts):\n",
    "    \"\"\"Update words & counts dictionaries\"\"\"\n",
    "    if word != '':\n",
    "        if word in words:\n",
    "            words[word] = words[word] * counts[word] + val\n",
    "            counts[word] += 1\n",
    "            words[word] /= float(counts[word])\n",
    "        else:\n",
    "            words[word] = val\n",
    "            counts[word] = 1\n",
    "    return words, counts\n",
    "\n",
    "\n",
    "def alltenses(v):\n",
    "    \"\"\"Return all different verb forms, given infinitive.\n",
    "\n",
    "    Uses the Linguistics module from Nodebox (called 'en').\n",
    "    \"\"\"\n",
    "    from pattern.en import conjugate\n",
    "    # filter out multi-word phrases, which we just ignore\n",
    "    if '_' in v:\n",
    "        return []\n",
    "    return list(set([conjugate(v, conj) for conj in \\\n",
    "                     ['inf', '1sg', '2sg', '3sg', 'pl', 'part', '1sgp', '2sgp', '3sgp', 'ppl', 'ppart']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SO-CAL lexicon from the official distribution, version 1.11.\n",
    "\n",
    "# Note: the pattern libraries conjugate function may raise an error\n",
    "# (StopIteration) the first time this is run. Just execute the cell again.\n",
    "\n",
    "socalfolder = SAfolder + 'SO-CAL/English (from GitHub)/'\n",
    "socalsuffix = '_dictionary1.11.txt'\n",
    "socalabbrevs = ['noun', 'verb', 'adj', 'adv', 'int']\n",
    "socalcats = ['noun', 'verb', 'adjective', 'adverb', 'intensifier']\n",
    "socalnames = {cat: abbrev + socalsuffix for cat, abbrev in zip(socalcats, socalabbrevs)}\n",
    "\n",
    "socallex, socalmods = importSOCAL(socalfolder, socalnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 2 entries with an accented letter that do not come through correctly:\n",
    "# cliché and 'scslsiscshs�s s'. The latter can be deleted (not sure what it is supposed to be). \n",
    "# The former should be corrected. Relatedly, there is an entry for clichd, which should be clichéd.\n",
    "\n",
    "badkey = ''\n",
    "for key, val in socallex.items():\n",
    "    if key[:5] == 'clich' and 'e' not in key and 'd' not in key:\n",
    "        badkey = key\n",
    "if len(badkey) > 0:\n",
    "    print('Deleting bad version of cliché')\n",
    "    del socallex[badkey]\n",
    "\n",
    "badkey = ''\n",
    "for key, val in socallex.items():\n",
    "    if key[:4] == 'scsl':\n",
    "        badkey = key\n",
    "if len(badkey) > 0:\n",
    "    print('Deleting bad \"scsl...\" term')\n",
    "    del socallex[badkey]\n",
    "\n",
    "socallex['cliché'] = -2  # Same as for 'cliche'\n",
    "socallex['clichés'] = -2  # Same as for 'cliches'\n",
    "\n",
    "if 'clichd' in socallex:\n",
    "    del socallex['clichd']\n",
    "socallex['clichéd'] = -3  # Same as for 'cliched'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify some additional fixes to the SO-CAL lexicon\n",
    "# Mostly these fix spelling errors\n",
    "\n",
    "socallex_fix = {# adjectives\n",
    "                'redepemption': '',\n",
    "                'suspensful': '',\n",
    "                'uncoventional': '',\n",
    "    \n",
    "                'anti-climatic': 'anti-climactic',\n",
    "                'autorcratic': 'autocratic',\n",
    "                'devestating': 'devastating',\n",
    "                'digruntled': 'disgruntled',\n",
    "                'disasterous': 'disastrous',\n",
    "                'forlon': 'forlorn',\n",
    "                'futurisitic': 'futuristic',\n",
    "                'inprudent': 'imprudent',\n",
    "                'intractible': 'intractable',\n",
    "                'juandiced': 'jaundiced',\n",
    "                'less-than-desireable': 'less-than-desirable',\n",
    "                'obsure': 'obscure',\n",
    "                'opressive': 'oppressive',\n",
    "                'plebian': 'plebeian',\n",
    "                'priviledged': 'privileged',\n",
    "                'pgnacious': 'pugnacious',\n",
    "                'strenous': 'strenuous',\n",
    "                'uneveness': 'unevenness',\n",
    "                'unweildy': 'unwieldy',\n",
    "                'uproductive': 'unproductive',\n",
    "                'inpudent': 'impudent',\n",
    "    \n",
    "                # adverbs\n",
    "                'immensly': 'immensely',  # this overwrites 'immensely' that was in there at valence 1\n",
    "                'exceptionaly': '',       # just delete\n",
    "                'digitaly': '',           # ,,\n",
    "                'entirly': 'entirely',\n",
    "                'realy': 'really',\n",
    "                'disasterously': 'disastrously',\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implement fix\n",
    "fix_lex(socallex, socallex_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the intensifier/modifier dictionary the same way\n",
    "\n",
    "socalmods_fix = {'a_mutltidue_of': 'a_multitude_of',\n",
    "                 'visable': 'visible',\n",
    "                 'collossal': 'colossal'}\n",
    "\n",
    "fix_lex(socalmods, socalmods_fix)\n",
    "\n",
    "# Change one entry\n",
    "socalmods['more'] = 0.5  # was -0.5 (i.e a weakening rather than a strengthening)\n",
    "\n",
    "# Add some additional entries\n",
    "socalmods['absence_of'] = -1.5 # Negater, not in SO-CAL lexicon\n",
    "socalmods['devoid_of'] = -1.5  # ,,\n",
    "socalmods['lack_of'] = -1.5    # ,,\n",
    "socalmods['not_very'] = -1.5   # Addition to parallel 'not_too' \n",
    "\n",
    "# Add some intensifiers from an earlier version of SO-CAL\n",
    "socalmods['low'] = -2.0\n",
    "socalmods['some'] = -0.2\n",
    "socalmods['obvious'] = 0.3\n",
    "socalmods['lots_of'] = 0.3\n",
    "socalmods['serious'] = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated versions. Add an 'X' to indicate they're modified\n",
    "\n",
    "with open(socalfolder + 'SO-CAL_lexiconX.csv', 'w') as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(socallex.items())))\n",
    "    \n",
    "with open(socalfolder + 'SO-CAL_modifiersX.csv', 'w') as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(socalmods.items())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. WordStat\n",
    "\n",
    "Provalis Research, the makers of WordStat, make available for public use a basic sentiment dictionary. This dictionary used to be located at http://www.provalisresearch.com/wordstat/Sentiment-Analysis.html (URL no longer live, but probably accessible via Wayback Machine), and that is the version we used through 2021. Our current set-up uses the newer version.\n",
    "\n",
    "The new version (2.0, dated 2018) is at https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/sentiment-dictionaries/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importWordStat(WSfolder, WSname, savepickle=False, savetext=True):\n",
    "    \"\"\"Import WordStat sentiment dictionary.\n",
    "\n",
    "    The dictionary file begins with several good/bad expressions; skip these because our\n",
    "    modifiers take care of them. This is followed by a list of negations (again, skip)\n",
    "    and a list of \"double negations\" (again, skip).\n",
    "\n",
    "    Finally, there are two sections titled \"NEGATIVE WORDS\" and \"POSITIVE WORDS\".\n",
    "    These we load into the WordStat dictionary.\n",
    "\n",
    "    The file is all caps, so make sure to lower-case things.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    import pickle\n",
    "\n",
    "    words = {}\n",
    "    poswords, negwords = 0, 0\n",
    "    negationwords, doublenegwords = [], []\n",
    "    curCat = 'prelims'\n",
    "\n",
    "    with open(WSfolder + WSname, 'r', errors='ignore') as infile:\n",
    "        for aWord in infile.readlines():\n",
    "            aWord = aWord.lower().strip()\n",
    "            if aWord == 'negations':\n",
    "                curCat = 'negs'\n",
    "            elif aWord == 'double_negation':\n",
    "                curCat = 'doublenegs'\n",
    "            elif aWord == 'positive words':\n",
    "                curCat = 'positive'\n",
    "            elif aWord == 'negative words':\n",
    "                curCat = 'negative'\n",
    "            elif aWord == 'exceptions':\n",
    "                curCat = 'exceptions'\n",
    "            elif curCat in ('positive', 'negative', 'negs', 'doublenegs'):\n",
    "                aWord = aWord[:-4]    # remove ' (1)' at end of each line\n",
    "                if curCat == 'negs':  # We don't actually do anything with negs and doublenegs, so could skip this\n",
    "                    negationwords.append(aWord)\n",
    "                elif curCat == 'doublenegs':\n",
    "                    doublenegwords.append(aWord.replace(\"_\", \" \"))\n",
    "                elif curCat == 'positive':\n",
    "                    words[aWord] = 1\n",
    "                    poswords += 1\n",
    "                else: # curCat == 'negative'\n",
    "                    words[aWord] = -1\n",
    "                    negwords += 1\n",
    "\n",
    "    print('Dictionary contained {} positive and {} negative words (including wildcards) for a total of {} words'.format(\n",
    "            poswords, negwords, len(words)))\n",
    "\n",
    "    if savepickle:\n",
    "        # Optionally add in the negationwords and doublenegwords here\n",
    "        with open('WordStatDictionary.pkl', 'wb') as outfile:\n",
    "            pickle.dump(words, outfile)\n",
    "    if savetext:\n",
    "        with open('WordStat_lexicon.csv', 'w') as outfile:\n",
    "            outwriter = csv.writer(outfile)\n",
    "            outwriter.writerows(sorted(list(words.items())))\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WordStat lexicon (version 2.0)\n",
    "wordstatfolder = SAfolder + 'WordStat/WSD 2.0/'\n",
    "wordstatfile = 'WordStat Sentiments.cat'\n",
    "\n",
    "wslex2 = importWordStat(wordstatfolder, wordstatfile, savepickle=False, savetext=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0 has phrases, with underscores; remove these\n",
    "keys2delete = []\n",
    "for key, val in wslex2.items():\n",
    "    if '_' in key:\n",
    "        keys2delete.append(key)\n",
    "print('Deleting {} entries with underscores (phrases)'.format(len(keys2delete)))\n",
    "for key in keys2delete:\n",
    "    del wslex2[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It also has a few lines beginning with @ that are comments; remove these too\n",
    "keys2delete = []\n",
    "for key, val in wslex2.items():\n",
    "    if key[0] == '@':\n",
    "        keys2delete.append(key)\n",
    "print('Deleting {} entries starting with @ (comments/notes)'.format(len(keys2delete)))\n",
    "for key in keys2delete:\n",
    "    del wslex2[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out subsumptions (if they have the same valence) and save\n",
    "\n",
    "wslex2 = lex_removesubsumed(wslex2)\n",
    "\n",
    "with open(wordstatfolder + 'WordStat_lexicon2X.csv', 'w') as outfile:\n",
    "    outwriter = csv.writer(outfile)\n",
    "    outwriter.writerows(sorted(list(wslex2.items())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
