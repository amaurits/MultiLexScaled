{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLexScaled - sentiment analysis (file-based) (2021-12-10)\n",
    "\n",
    "_by A. Maurits van der Veen_  \n",
    "\n",
    "_Modification history:_  \n",
    "_2021-12-03 - Convert to csv lexica; use newest versions of lexica, as publicly available_  \n",
    "_2021-12-10 - Clean up & streamline for GitHub repo_  \n",
    "\n",
    "This notebook applies sentiment analysis to a corpus. The corpus file-based and does not need to fit into memory all at once. Intermediate stages (cleaned text, individual valences, individual calibrated valences) are stored as separate output files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set-up\n",
    "\n",
    "Import necessary code modules; specify location of sentiment analysis lexica and associated files; specify corpus location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAIRfolder = '/Users/username/STAIR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code files to import\n",
    "import sys\n",
    "sys.path.append(STAIRfolder + 'Code')\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# local code modules -> these should be in the folder just specified (or otherwise locatable by python)\n",
    "import tokenization\n",
    "import valence\n",
    "import calibrate\n",
    "\n",
    "# Print summary version info (for fuller info, simply print sys.version)\n",
    "print('You are using python version {}.'.format(sys.version.split()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify where to find the sentiment analysis lexica and the calibration file, along with their names. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAfolder = STAIRfolder + 'Corpora/Lexica/English/MultiLexScaled/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexica = {'HuLiu':          SAfolder + 'HuLiu/opinion-lexicon-English/HuLiu_lexiconX.csv',\n",
    "          'LabMT_filtered': SAfolder + 'labMT/labMT_lexicon_filtered.csv',\n",
    "          'LexicoderSD':    SAfolder + 'Lexicoder/LSDaug2015/LSD_lexiconX.csv',\n",
    "          'MPQA':           SAfolder + 'MPQA 2.0/opinionfinderv2.0/lexicons/MPQA_lexicon.csv',\n",
    "          'NRC':            SAfolder + 'NRC/NRC-Emotion-Lexicon-v0.92/NRC_lexicon.csv',\n",
    "          'SOCAL':          SAfolder + 'SO-CAL/English (from GitHub)/SO-CAL_lexiconX.csv',\n",
    "          'SWN_filtered':   SAfolder + 'SWN/SWN_lexicon_filtered0.1.csv',\n",
    "          'WordStat':       SAfolder + 'WordStat/WSD 2.0/WordStat_lexicon2X.csv',\n",
    "         } \n",
    "lexnames = sorted(lexica.keys())\n",
    "\n",
    "# If not using modifiers, just set modifierlex to None\n",
    "modifierlex = SAfolder + 'SO-CAL/English (from GitHub)/SO-CAL_modifiersX.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexica & modifier info\n",
    "lexica_used = [valence.load_lex(lexfile) for lexname, lexfile in sorted(lexica.items())]\n",
    "mods = valence.load_lex(modifierlex) if len(modifierlex) > 0 else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the calibration pathname\n",
    "calibrationfolder = SAfolder + 'Calibration/'\n",
    "calibrationfile = calibrationfolder + 'Calibration_US_2021-12-10.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Corpus location & file names\n",
    "\n",
    "Identify the folder in which the corpus is to be found (and into which any new files will be saved).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify corpus location\n",
    "projectfolder = STAIRfolder1 + 'Corpora/Media/Neutral/Corpus/US/'\n",
    "corpusfilestem = projectfolder + 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocess text\n",
    "\n",
    "Pre-tokenize text to make sure punctuation does not affect sentiment calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clean file(s) from dataset\n",
    "# The default output is a file with the suffix _clean that contains 2 columns (id, cleanedtext) and no headers\n",
    "# See the code in tokenization.py for other options\n",
    "\n",
    "textcols = (10, 12)  # columns containing text (will be combined)\n",
    "rawsuffix = '_dedup'\n",
    "cleansuffix = '_clean'\n",
    "\n",
    "tokenization.preprocess_texts(corpusfilestem + rawsuffix + '.csv', \n",
    "                              corpusfilestem + cleansuffix + '.csv',\n",
    "                              textcols=textcols, inheader=True, lang='english',\n",
    "                              stripspecial=False, stripcomma=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate valence\n",
    "\n",
    "#### 2.1. Specify parameters\n",
    "\n",
    "We can specify words to ignore (for example, key search terms that might also appear in a valence lexicon), as well as special punctuation to skip (standard punctuation will be skipped automatically). The latter will not be included in the word count; the former will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignorewords = set()                  # Valenced words to ignore, if any, but include in wordcount\n",
    "words2skip = set(('.', ',', '...'))  # Words to skip altogether (usually just punctuation)\n",
    "\n",
    "# Negation words, to combine with modifiers/intensifiers such as 'very' or 'hardly' in adjusting valence\n",
    "negaters = ('not', 'no', 'neither', 'nor', 'nothing', 'never', 'none', \n",
    "            'nowhere', 'noone', 'nobody',\n",
    "            'lack', 'lacked', 'lacking', 'lacks', 'missing', 'without')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Valence calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansuffix = '_clean'\n",
    "cleantextcols = (1,)\n",
    "valencesuffix = '_vals'  # suffix for file to contain text-level valence data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusfile = corpusfilestem + cleansuffix + '.csv' \n",
    "valencefile = corpusfilestem + valencesuffix + '.csv'\n",
    "\n",
    "valence.calc_corpus_valence(corpusfile, valencefile, \n",
    "                            lexnames, lexica_used, mods, \n",
    "                            textcols=cleantextcols, modify=True, negaters=negaters, \n",
    "                            ignore=ignorewords, skip=words2skip, header=False,\n",
    "                            need2tokenize=False, makelower=True, skippunct=True,\n",
    "                            nrjobs=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calibrate\n",
    "\n",
    "Now we calibrate our valences. We can either calibrate against the parameters calculated from another corpus we assume to be neutral, or we can calibrate against ourselves, simply standardizing to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "To calibrate against an existing set of calibration parameters, set `extcalibrate` to be `True`, and specify the calibrationfile. The calibration file will contain the scaling parameters (mean, std. dev.) for each individual lexicon, as well as the standard deviation of their average, which we need to divide by as the final calibration step. The code snippet below loads the scaler and displays some information about it.\n",
    "\n",
    "To calibrate a corpus against itself (as here), set `extcalibrate` to `False`. If we want to use the resulting calibration parameters for additional corpora, set savescaler to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration file, as needed. Set to False to calibrate based on each corpus itself\n",
    "extcalibrate = False\n",
    "\n",
    "if extcalibrate:\n",
    "    # Load calibration data\n",
    "    neutralscaler, featurenames, nrfeatures, nravailable, stdev_adj, descriptor = \\\n",
    "            calibrate.load_scaler_fromcsv(calibrationfile, includevar=True, displayinfo=True)\n",
    "else:\n",
    "    neutralscaler = ''  # dummy value\n",
    "    stdev_adj = 1       # if not using pre-set calibration, also don't do any scale adjustment\n",
    "    scalersuffix = '_newscaler'\n",
    "    print('No scaler loaded -> will calibrate corpus against itself.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibratedsuffix = '_cal' # suffix for file containing calibrated valence data\n",
    "\n",
    "wordcountcol = 'nrwords'\n",
    "keepcols = []  # word count info is automatically retained, because used as a filter/scaler\n",
    "\n",
    "valencefile = corpusfilestem + valencesuffix + '.csv'\n",
    "\n",
    "scaler, new_stdev_adj, nrtexts = \\\n",
    "        calibrate.calibrate_features(valencefile, lexnames, \n",
    "                                     neutralscaler, stdev_adj=stdev_adj,\n",
    "                                     filtercol=wordcountcol, keepcols=keepcols,\n",
    "                                     missing=-999, outsuffix=calibratedsuffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save this scaler\n",
    "# (give temporary name & descriptor; can always change later)\n",
    "\n",
    "savescaler = False  # set to True to save the newly generated scaler\n",
    "scalername = 'newcorpus'\n",
    "newcalibrationfile = calibrationfolder + 'Calibration_new_temporary.csv'\n",
    "\n",
    "if savescaler:\n",
    "    descriptor = 'New scaler for {} based on {} texts. Generated: {}'.format(\n",
    "                     scalername, nrtexts, datetime.now())\n",
    "    calibrate.write_scaler_tocsv(newcalibrationfile, newscaler, featurenames=lexnames,\n",
    "                                 name=scalername, descriptor=descriptor, stdev_adj=new_stdev_adj)\n",
    "\n",
    "    # Describe the scaler (by reloading it)\n",
    "    print('\\nScaler pathname: {}\\n'.format(calibrationfile))\n",
    "    neutralscaler, featurenames, nrfeatures, nravailable, stdev_adj, descriptor = \\\n",
    "            calibrate.load_scaler_fromcsv(calibrationfile, includevar=True, displayinfo=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
