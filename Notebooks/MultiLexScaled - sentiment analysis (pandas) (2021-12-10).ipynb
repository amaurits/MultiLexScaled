{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLexScaled - sentiment analysis (pandas) (2021-12-10)\n",
    "\n",
    "_by A. Maurits van der Veen_  \n",
    "\n",
    "_Modification history:_  \n",
    "_2021-12-03 - Convert to csv lexica; use newest versions of lexica, as publicly available_  \n",
    "_2021-12-10 - Clean up & streamline for GitHub repo_  \n",
    "\n",
    "This notebook applies sentiment analysis to a corpus. The corpus is loaded into a pandas dataframe and all the calculations are done on the dataframe. Depending on memory and processing power, for corpora larger than 50,000 texts or so it may be preferable to use the file-based version of sentiment analysis, which saves intermediate stages (cleaned text, individual valences, individual calibrated valences) as separate output files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set-up\n",
    "\n",
    "Import necessary code modules; specify location of sentiment analysis lexica and associated files; specify corpus location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAIRfolder = '/Users/username/STAIR/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code files to import\n",
    "import sys\n",
    "sys.path.append(STAIRfolder + 'Code')\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# local code modules -> these should be in the folder just specified (or otherwise locatable by python)\n",
    "import tokenization\n",
    "import valence\n",
    "import calibrate\n",
    "\n",
    "# Print summary version info (for fuller info, simply print sys.version)\n",
    "print('You are using python version {}.'.format(sys.version.split()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify where to find the sentiment analysis lexica and the calibration file, along with their names. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAfolder = STAIRfolder + 'Corpora/Lexica/English/MultiLexScaled/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexica = {'HuLiu':          SAfolder + 'HuLiu/opinion-lexicon-English/HuLiu_lexiconX.csv',\n",
    "          'LabMT_filtered': SAfolder + 'labMT/labMT_lexicon_filtered.csv',\n",
    "          'LexicoderSD':    SAfolder + 'Lexicoder/LSDaug2015/LSD_lexiconX.csv',\n",
    "          'MPQA':           SAfolder + 'MPQA 2.0/opinionfinderv2.0/lexicons/MPQA_lexicon.csv',\n",
    "          'NRC':            SAfolder + 'NRC/NRC-Emotion-Lexicon-v0.92/NRC_lexicon.csv',\n",
    "          'SOCAL':          SAfolder + 'SO-CAL/English (from GitHub)/SO-CAL_lexiconX.csv',\n",
    "          'SWN_filtered':   SAfolder + 'SWN/SWN_lexicon_filtered0.1.csv',\n",
    "          'WordStat':       SAfolder + 'WordStat/WSD 2.0/WordStat_lexicon2X.csv',\n",
    "         } \n",
    "lexnames = sorted(lexica.keys())\n",
    "\n",
    "# If not using modifiers, just set modifierlex to None\n",
    "modifierlex = SAfolder + 'SO-CAL/English (from GitHub)/SO-CAL_modifiersX.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexica & modifier info\n",
    "lexica_used = [valence.load_lex(lexfile) for lexname, lexfile in sorted(lexica.items())]\n",
    "mods = valence.load_lex(modifierlex) if len(modifierlex) > 0 else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the calibration pathname\n",
    "calibrationfolder = SAfolder + 'Calibration/'\n",
    "calibrationfile = calibrationfolder + 'Calibration_US_2021-12-10.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Corpus location & file names\n",
    "\n",
    "Identify the folder in which the corpus is to be found (and into which any new files will be saved).\n",
    "This notebook is set up to handle several different subcorpora for which we're interested in the same things. Optionally, we combine them in the end.\n",
    "\n",
    "To use just a single corpus, simply make `corpusnames` a one-item list (do put a comma after its name, to make sure Python realizes it is a 1-item list). Each (sub-) corpus name should correspond to the name of an existing folder inside `projectfolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input corpus\n",
    "corpusfile = STAIRfolder + 'Corpora/Media/Neutral/Corpus/US/US_dedup.csv'\n",
    "\n",
    "# Load into dataframe\n",
    "df = pd.read_csv(corpusfile, index_col=False)\n",
    "df.rename(columns={'DocNr': 'id',}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocess text\n",
    "\n",
    "Pre-tokenize text to make sure punctuation does not affect sentiment calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clean file(s) from dataset\n",
    "# The default output is a file with the suffix _clean that contains 2 columns (id, cleanedtext) and no headers\n",
    "# See the code in tokenization.py for other options\n",
    "\n",
    "textcols = ('Title', 'Text')  # columns containing text (will be combined)\n",
    "\n",
    "df['cleantext'] = df.apply(lambda row: tokenization.punctuationPreprocess(' . '.join([str(row[col]) for col in textcols])), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate valence\n",
    "\n",
    "#### 2.1. Specify parameters\n",
    "\n",
    "We can specify words to ignore (for example, key search terms that might also appear in a valence lexicon), as well as special punctuation to skip (standard punctuation will be skipped automatically). The latter will not be included in the word count; the former will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignorewords = set()                  # Valenced words to ignore, if any, but include in wordcount\n",
    "words2skip = set(('.', ',', '...'))  # Words to skip altogether (usually just punctuation)\n",
    "\n",
    "# Negation words, to combine with modifiers/intensifiers such as 'very' or 'hardly' in adjusting valence\n",
    "negaters = ('not', 'no', 'neither', 'nor', 'nothing', 'never', 'none', \n",
    "            'nowhere', 'noone', 'nobody',\n",
    "            'lack', 'lacked', 'lacking', 'lacks', 'missing', 'without')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of all keys across our lexica, but remove ignorewords\n",
    "allterms = valence.allkeys(lexica_used)\n",
    "ignoreX = set(ignorewords) - allterms  # Words to skip separately because not in any lexica\n",
    "allterms -= set(ignorewords)  # Update allterms to ignore words in our ignore set\n",
    "\n",
    "# Generate flags indicating whether a lexicon has wildcards\n",
    "wildlexicon = [valence.haswilds(lex) for lex in lexica_used]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Valence calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns of interest\n",
    "idcol = 'id'\n",
    "textcols = ('cleantext',)\n",
    "\n",
    "valence_df = valence.calc_valences(df, idcol, textcols, lexnames, lexica_used, \n",
    "                                   wildlexicon, allterms, mods,\n",
    "                                   modify=True, negaters=negaters,\n",
    "                                   ignore=ignoreX, skip=words2skip,\n",
    "                                   need2tokenize=False,  # Text already cleaned does not need to be tokenized\n",
    "                                   makelower=True, skippunct=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calibrate\n",
    "\n",
    "Now we calibrate our valences. We can either calibrate against the parameters calculated from another corpus we assume to be neutral, or we can calibrate against ourselves, simply standardizing to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "To calibrate against an existing set of calibration parameters, set `extcalibrate` to be `True`, and specify the calibrationfile. The calibration file will contain the scaling parameters (mean, std. dev.) for each individual lexicon, as well as the standard deviation of their average, which we need to divide by as the final calibration step. The code snippet below loads the scaler and displays some information about it.\n",
    "\n",
    "To calibrate a corpus against itself (as here), set `extcalibrate` to `False`. If we want to use the resulting calibration parameters for additional corpora, set savescaler to `True`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Load & apply scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration file, as needed. Set to False to calibrate based on each corpus itself\n",
    "extcalibrate = True\n",
    "\n",
    "if extcalibrate:\n",
    "    # Load calibration data\n",
    "    neutralscaler, featurenames, nrfeatures, nravailable, stdev_adj, descriptor = \\\n",
    "            calibrate.load_scaler_fromcsv(calibrationfile, includevar=True, displayinfo=True)\n",
    "else:\n",
    "    neutralscaler = ''  # dummy value\n",
    "    stdev_adj = 1       # if not using pre-set calibration, also don't do any scale adjustment\n",
    "    scalersuffix = '_newscaler'\n",
    "    print('No scaler loaded -> will calibrate corpus against itself.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_df, scaler, stdv_adj = \\\n",
    "    calibrate.calibrate_valencedata(valence_df, lexnames, neutralscaler, stdev_adj,\n",
    "                                    filtercol='nrwords', missing=-999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save this scaler\n",
    "# (give temporary name & descriptor; can always change later)\n",
    "\n",
    "savescaler = False  # set to True to save the newly generated scaler\n",
    "scalername = 'newcorpus'\n",
    "newcalibrationfile = calibrationfolder + 'Calibration_new_temporary.csv'\n",
    "\n",
    "if savescaler:\n",
    "    descriptor = 'New scaler for {} based on {} texts. Generated: {}'.format(\n",
    "                     scalername, nrtexts, datetime.now())\n",
    "    calibrate.write_scaler_tocsv(newcalibrationfile, scaler, featurenames=lexnames,\n",
    "                                 name=scalername, descriptor=descriptor, stdev_adj=stdv_adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Drop individual lexicon valence data, and merge into original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(calibrated_df[['id', 'avg_valence']], on = 'id', how = 'left')\n",
    "len(df)  # Double-check length still the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results, if desired\n",
    "outputfile = STAIRfolder + 'Corpora/Media/Neutral/Corpus/US/US_with_clean&valence.csv'\n",
    "df.to_csv(outputfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
